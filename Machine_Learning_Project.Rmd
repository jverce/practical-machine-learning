---
title: "Machine Learning Project"
author: "J Vercellone"
date: '2018-02-02'
output:
  md_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The purpose of this document is to describe the process of creating a machine learning model to predict the class of exercise from certain motion variables. The original experiment is called **Human Activity Recognition** and more information can be found here:
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

The experiment basically took place by adding several motion detectors in different parts of the human body and recording their output during specific gym routines. These routines where performed by different subjects and in a supervised way. The main goal of the original experiment is to determine which exercise a user is performing and whether or not it's properly performing it.

In our case we'll be using the training data acquired in the original experiment and try to build a machine learning model that predicts the class of exercise being performed based on the motion variables.

## Data Cleanup

In this section we'll be performing a data cleanup since the original files contain a lot of undefined information that can negatively impact our analysis.

### Load Data

We start by loading both the training and testing data and storing it in the `training` and `testing` variables, respectively:
```{r load}
training <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
testing <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
```

We also take the chance to load the libraries now:
```{r library, results='hide'}
library(caret)
library(randomForest)
```

### Glimpse

We take a quick glimpse at our data to find some inconsistent/improper variables to discard. _Keep in mind that we're hiding the results from this report since it's too verbose, but they're still reproducible_.

```{r colnames, results='hide'}
colnames(training)
colnames(testing)
```

We can see that the `classe` variable is missing from the testing set, which means that we won't be able to evaluate the models we create unless we partition the training set and work with that.

```{r head, results='hide'}
head(training)
```

```{r summary, results='hide'}
summary(training)
```

Based on the output we can find the following variables to remove:

- `X`: it's a variable that indicates the record number and should definitely be removed prior to our analysis
- `user_name`: similar to the previous variable, the user name should not be used as a predictor for a general case.
- `new_window`: this variable presents inconsistencies in its values and it's better to leave it out.
- Timestamp variables: these are variables that contain `timestamp` in their name. They do not provide information about the subject's movement and so they shouldn't affect our prediction.
- `NA` variables: these variables have undefined values and since we don't have any way to supply them we have no option but to remove them.
- `#DIV/0!` variables: looks like variables that experienced some strange issue during their recording and we can't rely on their values, hence we'll be removing them (we can identify them as "factor" variables).

### Variable Removal

As per the previous section, we proceed to remove the mentioned variables:
```{r cleanup}
# Remove `X` var
training <- subset(training, select = -c(X))

# Remove `user_name` var
training <- subset(training, select = -c(user_name))

# Remove `new_window` var
training <- subset(training, select = -c(new_window))

# Remove timestamp vars
training <- subset(training, select = -grep('timestamp', colnames(training)))

# Remove `NA` vars
training <- training[, (colSums(is.na(training)) == 0)]

# Remove `#DIV/0!` vars (be careful not to remove the output var which is a factor)
predictors <- which(lapply(training, class) %in% c('integer', 'numeric'))
y <- which(colnames(training) == 'classe')
training <- training[, c(predictors, y)]
```

Finally we see how many variables are left in our training set:
```{r dim}
dim(training)[2]
```

We now proceed with the data partition:
```{r createDataPartition}
inTrain <- createDataPartition(training$classe, p = .7, list = FALSE)
trainingSet <- training[inTrain, ]
testingSet <- training[-inTrain, ]
```

## Build Model

To build our machine learning model we'll be using the _random forest_ algorithm since it provides a fair amount of accuracy. The only aspect we need to be aware of is the possibility of overfitting. We can play around with the `mtry` variable in order to balance between the accuracy against the training set and the amount of variables sampled as candidates: we can reduce overfitting and simplifying the model at the same time by reducing the `mtry` value we provide as input.

We try with the following `mtry` values:

- 2 (minimum)
- 27 (half the max)
- 53 (maximum)

```{r randomForest}
randomForest(classe ~ ., data = trainingSet, mtry = 2)
randomForest(classe ~ ., data = trainingSet, mtry = 27)
randomForest(classe ~ ., data = trainingSet, mtry = 53)
```

We can see that the best performance is achieved with `mtry = 27`. However, even with `mtry = 2` we have a great performance as well, with a simpler processing. Hence we'll be choosing this last option for our algorithm.

We can now do a prediction to check if the output looks consistent:
```{r predict}
fit <- randomForest(classe ~ ., data = trainingSet, mtry = 2)
predictions <- predict(fit, testingSet)

cm <- confusionMatrix(predictions, testingSet$classe)
print(cm)
```

We can calculate the **out-of-sample** error based on the confusion matrix data we just obtained:
```{r out-of-sample}
1 - cm$overall[[1]]
```

As predicted, this error is very low so we're confident about performing predictions in the original test set. This can also be verified with against the **final quiz** of the course:
```{r test-predict}
predict(fit, testing)
```

As per the **final quiz**, the result is 100% accurate.

## Conclusions

As seen before, it is very important to perform a data inspection prior to any analysis since there might be dummy, dirty or auxiliary variables that are not consistent with the processing and information we need to extract.

We observed that by applying the random forest algorithm we were able to obtain a prediction model that is accurate enough without overfitting the initial training data. Even though we used the `mtry` parameter to balance the precision and complexity of the model, there are other parameters that can be adapted as well (check the official `randomForest` [documentation](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) for more information).

Finally we used the obtained model to perform a prediction on the tesing dataset with 100% accuracy according to the final quiz of the course.